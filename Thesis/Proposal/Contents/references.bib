@article{DroneschasingDrones,
  title={Drones Chasing Drones: Reinforcement Learning and Deep Search Area Proposal},
  author={Akhloufi, Moulay A and Arola, Sebastien and Bonnet, Alexandre},
  journal={Drones},
  volume={3},
  number={3},
  pages={58},
  year={2019},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@article{DroneRLUsingTransferLearning,
  title={Autonomous Navigation via Deep Reinforcement Learning for Resource Constraint Edge Nodes Using Transfer Learning},
  author={Anwar, Aqeel and Raychowdhury, Arijit},
  journal={IEEE Access},
  volume={8},
  pages={26549--26560},
  year={2020},
  publisher={IEEE}
}
@misc{TransferLearningGithub,
  author = {aqeelanwar},
  title = {Deep Reinforcement Learning with Transfer Learning - Simulated Drone and Environment},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished={\url{https://www.github.com/aqeelanwar/DRLwithTL}}
}

@misc{stereopi, 
title = {StereoPi},
howpublished={\url{https://stereopi.com/#home}}, 
journal={StereoPi}, 
year={2015}
}
@misc{TFAgents,
  title = { {TF-Agents}: A library for Reinforcement Learning in TensorFlow},
  author = {Sergio Guadarrama and Anoop Korattikara and Oscar Ramirez and
     Pablo Castro and Ethan Holly and Sam Fishman and Ke Wang and
     Ekaterina Gonina and Neal Wu and Efi Kokiopoulou and Luciano Sbaiz and
     Jamie Smith and Gábor Bartók and Jesse Berent and Chris Harris and
     Vincent Vanhoucke and Eugene Brevdo},
  howpublished={\url{"https://github.com/tensorflow/agents"}},
  year = 2018,
  note = "[Online; accessed 25-June-2019]"
}
@article{mAP,
  title={The pascal visual object classes (voc) challenge},
  author={Everingham, Mark and Van Gool, Luc and Williams, Christopher KI and Winn, John and Zisserman, Andrew},
  journal={International journal of computer vision},
  volume={88},
  number={2},
  pages={303--338},
  year={2010},
  publisher={Springer}
}
@book{RLBook, 
  place={Massachusetts}, 
  title={Reinforcement Learning: An Introduction}, 
  publisher={MIT Press Ltd}, 
  author={Sutton, Richard S. and Bach, Francis and Barto, Andrew G.}, 
  year={2018}
}
@article{application1cardiac,
  title={Unmanned aerial vehicles (drones) in out-of-hospital-cardiac-arrest},
  author={Claesson, A and Fredman, D and Svensson, L and Ringh, M and Hollenberg, J and Nordberg, P and Rosenqvist, M and Djarv, T and {\"O}sterberg, S and Lennartsson, J and others},
  journal={Scandinavian journal of trauma, resuscitation and emergency medicine},
  volume={24},
  number={1},
  pages={124},
  year={2016},
  publisher={Springer}
}
@article{application2forestfires,
  title={Early forest fire detection and verification using optical smoke, gas and microwave sensors},
  author={Kr{\"u}ll, Wolfgang and Tobera, Robert and Willms, Ingolf and Essen, Helmut and von Wahl, Nora},
  journal={Procedia Engineering},
  volume={45},
  pages={584--594},
  year={2012},
  publisher={Elsevier}
}

@misc{embeddedsystem, title={What is an Embedded System? Definition and FAQs}, 
  howpublished={\url{https://www.omnisci.com/technical-glossary/embedded-systems}}, 
  journal={OmniSci}, 
  publisher={OmniSci}
}

@misc{neuralnets, 
  title={Explained: Neural networks}, 
  howpublished={\url{https://news.mit.edu/2017/explained-neural-networks-deep-learning-0414}}, 
  journal={MIT News | Massachusetts Institute of Technology}, 
  publisher={MIT News Office}, 
  author={Hardesty, Larry}, 
  year={2017}, 
  month={Apr}
}

@article{firstcnn,
issn = {1573-1405},
abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
journal = {International journal of computer vision},
pages = {211--252},
volume = {115},
publisher = {Springer Science and Business Media LLC},
number = {3},
year = {2015},
title = {ImageNet Large Scale Visual Recognition Challenge},
copyright = {Springer Science+Business Media New York 2015},
language = {eng},
address = {New York},
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C and Fei-Fei, Li},
keywords = {Pattern Recognition ; Large-scale ; Dataset ; Computer Science ; Computer Imaging, Vision, Pattern Recognition and Graphics ; Image Processing and Computer Vision ; Object detection ; Artificial Intelligence (incl. Robotics) ; Benchmark ; Object recognition ; Machine vision},
}
@article{iowamasterthesis,
  title={Visual object tracking for UAVs using deep reinforcement learning},
  author={Ko, Kyungtae},
  year={2020}
}

@article{lidarinselfdrivingcar,
issn = {1047-6938},
journal = {Optics and photonics news},
pages = {26},
volume = {29},
number = {1},
year = {2018},
title = {Lidar for Self-Driving Cars},
language = {eng},
author = {Hecht, Jeff},
}



@article{fasterrcnn,
issn = {0162-8828},
abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network(RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features-using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3], our detection system has a frame rate of 5 fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
pages = {1137--1149},
volume = {39},
publisher = {IEEE},
number = {6},
year = {2017},
title = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
copyright = {Copyright 2017 Elsevier B.V., All rights reserved.},
language = {eng},
address = {United States},
author = {Shaoqing Ren and Kaiming He and Girshick, Ross and Jian Sun},
keywords = {Convolutional codes ; Training ; convolutional neural network ; Object detection ; Detectors ; Feature extraction ; Search problems ; Proposals ; region proposal ; Usage ; Algorithms ; Neural networks ; Analysis},
}



@article{fastrcnn,
  author    = {Ross B. Girshick},
  title     = {Fast {R-CNN}},
  journal   = {CoRR},
  volume    = {abs/1504.08083},
  year      = {2015},
  howpublished={\url{http://arxiv.org/abs/1504.08083}},
  archivePrefix = {arXiv},
  eprint    = {1504.08083},
  timestamp = {Mon, 13 Aug 2018 16:49:11 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/Girshick15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{original-rcnn,
issn = {1063-6919},
abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.},
pages = {580--587},
publisher = {IEEE},
isbn = {9781479951185},
year = {2014},
title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
language = {eng},
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
keywords = {Training ; Support vector machines ; Visualization ; Object detection ; Feature extraction ; Vectors ; Proposals},
}



@article{application3sports,
  title={Motion tracking drone for extreme sports filming},
  author={Iastrebov, Viatcheslav and Wong, Choon Yue and Pang, Wee Ching and Seet, Gerald},
  year={2014}
}

@inproceedings{ParrotARDrone,
  title={Any object tracking and following by a flying drone},
  author={Bartak, Roman and Vykovsk{\`y}, Adam},
  booktitle={2015 Fourteenth Mexican International Conference on Artificial Intelligence (MICAI)},
  pages={35--41},
  year={2015},
  organization={IEEE}
}

@article{YOLOv4,
  title={YOLOv4: Optimal Speed and Accuracy of Object Detection},
  author={Bochkovskiy, Alexey and Wang, Chien-Yao and Liao, Hong-Yuan Mark},
  journal={arXiv preprint arXiv:2004.10934},
  year={2020}
}
@inproceedings{AirSimDroneNavigation,
  title={Drone Navigation and Avoidance of Obstacles Through Deep Reinforcement Learning},
  author={{\c{C}}etin, Ender and Barrado, Cristina and Mu{\~n}oz, Guillem and Macias, Miquel and Pastor, Enric},
  booktitle={2019 IEEE/AIAA 38th Digital Avionics Systems Conference (DASC)},
  pages={1--7},
  year={2019},
  organization={IEEE}
}
@article{TakeOffFlyForwardusingRL,
  title={Accuracy Improvement of Autonomous Straight Take-off, Flying Forward, and Landing of a Drone with Deep Reinforcement Learning},
  author={Chang, Che-Cheng and Tsai, Jichiang and Lu, Peng-Chen and Lai, Chuan-An},
  journal={International Journal of Computational Intelligence Systems},
  volume={13},
  number={1},
  pages={914--919},
  year={2020},
  publisher={Atlantis Press}
}
@inproceedings{ObstacleAvoidance,
  title={Development of autonomous drones for adaptive obstacle avoidance in real world environments},
  author={Devos, Arne and Ebeid, Emad and Manoonpong, Poramate},
  booktitle={2018 21st Euromicro Conference on Digital System Design (DSD)},
  pages={707--710},
  year={2018},
  organization={IEEE}
}
@article{Visual-GPS,
  title={Visual-GPS combined ‘follow-me’tracking for selfie drones},
  author={Do, T Tuan and Ahn, Heejune},
  journal={Advanced Robotics},
  volume={32},
  number={19},
  pages={1047--1060},
  year={2018},
  publisher={Taylor \& Francis}
}
@article{DeepRLforNavUsingSensor,
  title={Deep reinforcement learning for drone navigation using sensor data},
  author={Hodge, Victoria J and Hawkins, Richard and Alexander, Rob},
  journal={Neural Computing and Applications},
  pages={1--19},
  year={2020},
  publisher={Springer}
}
@article{Hossain,
  title={Deep learning-based real-time multiple-object detection and tracking from aerial imagery via a flying robot with GPU-based embedded devices},
  author={Hossain, Sabir and Lee, Deok-jin},
  journal={Sensors},
  volume={19},
  number={15},
  pages={3371},
  year={2019},
  publisher={Multidisciplinary Digital Publishing Institute}
}
@inproceedings{haarcascades,
  title={Rapid object detection using a boosted cascade of simple features},
  author={Viola, Paul and Jones, Michael},
  booktitle={Proceedings of the 2001 IEEE computer society conference on computer vision and pattern recognition. CVPR 2001},
  volume={1},
  pages={I--I},
  year={2001},
  organization={IEEE}
}
@INPROCEEDINGS{HOGdetection,
  author={N. {Dalal} and B. {Triggs}},
  booktitle={2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)}, 
  title={Histograms of oriented gradients for human detection}, 
  year={2005},
  volume={1},
  number={},
  pages={886-893 vol. 1},
  doi={10.1109/CVPR.2005.177}}
@INPROCEEDINGS{yolov3-tiny,
  author={P. {Adarsh} and P. {Rathi} and M. {Kumar}},
  booktitle={2020 6th International Conference on Advanced Computing and Communication Systems (ICACCS)}, 
  title={YOLO v3-Tiny: Object Detection and Recognition using one stage improved model}, 
  year={2020},
  volume={},
  number={},
  pages={687-694},
  doi={10.1109/ICACCS48705.2020.9074315}}

@inproceedings{yolo9000,
  title={YOLO9000: better, faster, stronger},
  author={Redmon, Joseph and Farhadi, Ali},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={7263--7271},
  year={2017}
}

@article{yolov3,
  title={Yolov3: An incremental improvement},
  author={Redmon, Joseph and Farhadi, Ali},
  journal={arXiv preprint arXiv:1804.02767},
  year={2018}
}

@inproceedings{airsim,
  author = {Shital Shah and Debadeepta Dey and Chris Lovett and Ashish Kapoor},
  title = {AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles},
  year = {2017},
  booktitle = {Field and Service Robotics},
  eprint = {arXiv:1705.05065},
  howpublished={\url{https://arxiv.org/abs/1705.05065}}
}

@misc{unrealengine, 
  title={Unreal Engine: The most powerful real-time 3D creation platform}, 
  howpublished={\url{https://www.unrealengine.com/en-US/}}, 
  journal={Unreal Engine}
}

@misc{TPU, 
  title={Get started with the USB Accelerator}, 
  howpublished={\url{https://coral.ai/docs/accelerator/get-started/#requirements}}, 
  journal={Coral}, 
  author={Coral}
}

@misc{raspberrycam, 
  title={Buy a Camera Module V2}, 
  howpublished={\url{https://www.raspberrypi.org/products/camera-module-v2/?resellerType=home}}, 
  journal={Raspberry Pi}, 
  author={Raspberry Pi}, 
  year={2016}, 
  month={Apr}
}
@inproceedings{originalyolo,
  title={You only look once: Unified, real-time object detection},
  author={Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={779--788},
  year={2016}
}
@inproceedings{YOLO-Lite,
  title={YOLO-LITE: a real-time object detection algorithm optimized for non-GPU computers},
  author={Huang, Rachel and Pedoeem, Jonathan and Chen, Cuixian},
  booktitle={2018 IEEE International Conference on Big Data (Big Data)},
  pages={2503--2510},
  year={2018},
  organization={IEEE}
}
@article{cloudcomputingdrone,
  title={Dronetrack: Cloud-based real-time object tracking using unmanned aerial vehicles over the internet},
  author={Koub{\^a}a, Anis and Qureshi, Basit},
  journal={IEEE Access},
  volume={6},
  pages={13810--13824},
  year={2018},
  publisher={IEEE}
}
@article{RLenLSTMfordrone,
  title={End-to-end active object tracking and its real-world deployment via reinforcement learning},
  author={Luo, Wenhan and Sun, Peng and Zhong, Fangwei and Liu, Wei and Zhang, Tong and Wang, Yizhou},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={42},
  number={6},
  pages={1317--1332},
  year={2019},
  publisher={IEEE}
}
@article{acousticdronefollower,
  title={The development of an autonomous navigation system with optimal control of an UAV in partly unknown indoor environment},
  author={Mac, Thi Thoa and Copot, Cosmin and De Keyser, Robin and Ionescu, Clara M},
  journal={Mechatronics},
  volume={49},
  pages={187--196},
  year={2018},
  publisher={Elsevier}
}
@inproceedings{blindrunnersdrone,
  title={Exploring the use of a drone to guide blind runners},
  author={Al Zayer, Majed and Tregillus, Sam and Bhandari, Jiwan and Feil-Seifer, Dave and Folmer, Eelke},
  booktitle={Proceedings of the 18th International ACM SIGACCESS Conference on Computers and Accessibility},
  pages={263--264},
  year={2016}
}
@inproceedings{DroneFollowUsingPhone,
  title={Indoor follow me drone},
  author={Mao, Wenguang and Zhang, Zaiwei and Qiu, Lili and He, Jian and Cui, Yuchen and Yun, Sangki},
  booktitle={Proceedings of the 15th Annual International Conference on Mobile Systems, Applications, and Services},
  pages={345--358},
  year={2017}
}
@article{flightwithtrack,
  title={Flight Coordination of MAVs in GPS-denied Environments using a Metric Visual SLAM},
  author={Rojas-Perez, L Oyuki and Martinez-Carranza, J}
}
@article{DroneFollowMobileObject,
  title={Visual detection and tracking with UAVs, following a mobile object},
  author={Mercado-Ravell, Diego A and Castillo, Pedro and Lozano, Rogelio},
  journal={Advanced Robotics},
  volume={33},
  number={7-8},
  pages={388--402},
  year={2019},
  publisher={Taylor \& Francis}
}
@inproceedings{PowerlineFollower,
  title={Autonomous Drone-Based Powerline Insulator Inspection via Deep Learning},
  author={Muhammad, Anas and Shahpurwala, Adnan and Mukhopadhyay, Shayok and El-Hag, Ayman H},
  booktitle={Iberian Robotics conference},
  pages={52--62},
  year={2019},
  organization={Springer}
}
@article{FrontalViewRL,
  title={Continuous drone control using deep reinforcement learning for frontal view person shooting},
  author={Passalis, Nikolaos and Tefas, Anastasios},
  journal={Neural Computing and Applications},
  pages={1--12},
  year={2019},
  publisher={Springer}
}

@misc{stereovision, 
  title={Comparing Three Prevalent 3D Imaging Technologies}, 
  howpublished={https://www.revopoint3d.com/comparing-three-prevalent-3d-imaging-technologies-tof-structured-light-and-binocular-stereo-vision/}, 
  journal={Revopoint 3D Technologies Inc.}, 
  author={3D, About the Author: Revopoint},
  year={2019}, 
  month={Nov}
}

@inproceedings{DepthFromMonocularImage,
  title={Learning depth from single monocular images},
  author={Saxena, Ashutosh and Chung, Sung H and Ng, Andrew Y},
  booktitle={Advances in neural information processing systems},
  pages={1161--1168},
  year={2006}
}

 @misc{arduino, 
  title={Arduino}, 
  howpublished={\url{https://www.arduino.cc/}}, 
  journal={Arduino}
  } 

@article{rewardshapingcomplex,
  title={Data-efficient deep reinforcement learning for dexterous manipulation},
  author={Popov, Ivaylo and Heess, Nicolas and Lillicrap, Timothy and Hafner, Roland and Barth-Maron, Gabriel and Vecerik, Matej and Lampe, Thomas and Tassa, Yuval and Erez, Tom and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1704.03073},
  year={2017}
}

@article{pruning,
  title={Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  journal={arXiv preprint arXiv:1510.00149},
  year={2015}
}

@article{pruningmeta,
  title={What is the state of neural network pruning?},
  author={Blalock, Davis and Ortiz, Jose Javier Gonzalez and Frankle, Jonathan and Guttag, John},
  journal={arXiv preprint arXiv:2003.03033},
  year={2020}
}

@article{HER,
  title={Hindsight experience replay},
  author={Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Pieter Abbeel, OpenAI and Zaremba, Wojciech},
  journal={Advances in neural information processing systems},
  volume={30},
  pages={5048--5058},
  year={2017}
}

@techreport{rewardshaping,
  title={Theory and application of reward shaping in reinforcement learning},
  author={Laud, Adam Daniel},
  year={2004}
}

@article{rainbowrl,
  title={Rainbow: Combining improvements in deep reinforcement learning},
  author={Hessel, Matteo and Modayil, Joseph and Van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
  journal={arXiv preprint arXiv:1710.02298},
  year={2017}
}

@article{pretrainingfinetuning,
  title={A survey on image data augmentation for deep learning},
  author={Shorten, Connor and Khoshgoftaar, Taghi M},
  journal={Journal of Big Data},
  volume={6},
  number={1},
  pages={60},
  year={2019},
  publisher={Springer}
}

@incollection{stereovisiontheory,
  title = "2 - Basics of Autonomous Vehicles",
  editor = "Rahul Kala",
  booktitle = "On-Road Intelligent Vehicles",
  publisher = "Butterworth-Heinemann",
  pages = "11 - 35",
  year = "2016",
  isbn = "978-0-12-803729-4",
  doi = "https://doi.org/10.1016/B978-0-12-803729-4.00002-7",
  url = "http://www.sciencedirect.com/science/article/pii/B9780128037294000027",
  author = "Rahul Kala",
  keywords = "Autonomous vehicles, Control, Intelligent vehicles, Localization, Mobile robotics, Motion planning, Sensing, Vision",
  abstract = "The technology behind autonomous vehicles is interesting and challenging. The chapter, in a nutshell, discusses the complete technology and shows how autonomous vehicles get the capability to navigate autonomously in traffic scenarios. The hardware of autonomous vehicles, from a computational perspective, consists of sensors including vision cameras, RADARs, ultrasonics and LIDARs, along with an Inertial Measurement Unit and motion encoders to enable the vehicles estimate the position. The vehicles are driven with the help of steering, brake and throttle using the drive-by-wire technology which facilitates driving using computer programmes. The vision systems are responsible for looking at the operational scenario and making inferences, which are used to make the map of the world by a mapping module. The localization module uses the vision and map information to estimate the vehicle's pose. Motion-planning algorithms do all the decision-making including computing trajectories for operation, which are followed by using control algorithms."
}

@article{FastDepthandObstacleAvoidanceOnMonocularDrone,
  title={Fast Depth Prediction and Obstacle Avoidance on a Monocular Drone Using Probabilistic Convolutional Neural Network},
  author={Yang, Xin and Chen, Jingyu and Dang, Yuanjie and Luo, Hongcheng and Tang, Yuesheng and Liao, Chunyuan and Chen, Peng and Cheng, Kwang-Ting},
  journal={IEEE Transactions on Intelligent Transportation Systems},
  year={2019},
  publisher={IEEE}
}
@article{Mixed-Yolo_Lite,
  title={Mixed YOLOv3-LITE: A lightweight real-time object detection method},
  author={Zhao, Haipeng and Zhou, Yang and Zhang, Long and Peng, Yangzhao and Hu, Xiaofei and Peng, Haojie and Cai, Xinyue},
  journal={Sensors},
  volume={20},
  number={7},
  pages={1861},
  year={2020},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@misc{yolofastest,
    author = {qiuqiuqiu},
    year = {2020},
    title = {Yolo-Fastest},
    subtitle = {Fact Sheet N°282},
    howpublished={\url{https://zhuanlan.zhihu.com/p/234506503}},
    note = {Accessed = 2020} 
}
@article{DQNDeepmind,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{DDQN,
  title={Deep reinforcement learning with double q-learning},
  author={Van Hasselt, Hado and Guez, Arthur and Silver, David},
  journal={arXiv preprint arXiv:1509.06461},
  year={2015}
}